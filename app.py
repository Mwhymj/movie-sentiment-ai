import streamlit as st
import joblib
import pandas as pd
import numpy as np
from pythainlp.tokenize import word_tokenize
from functools import lru_cache

# --- 1. CONFIG ---
st.set_page_config(page_title="Movie Sentiment Analysis", layout="wide")

# --- 2. FAST FUNCTIONS ---
@lru_cache(maxsize=1000)
def thai_tokenize(text):
    return word_tokenize(str(text), engine='newmm')

@st.cache_resource
def load_assets():
    try:
        m1 = joblib.load('model.joblib')
        m2 = joblib.load('model_v2.joblib')
        df = pd.read_csv('8.synthetic_netflix_like_thai_reviews_3class_hard_5000.csv')
        return m1, m2, df
    except: return None, None, None

model_v1, model_v2, df = load_assets()

# --- 3. SIMPLE & CLEAN UI ---
st.title("üé¨ Thai Movie Sentiment Analysis")
st.write("‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏à‡∏≤‡∏Å‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏†‡∏≤‡∏û‡∏¢‡∏ô‡∏ï‡∏£‡πå (‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•)")

# ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô (‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô 100 ‡πÄ‡∏ï‡πá‡∏°)
with st.expander("üìù ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Technical Details)"):
    col_a, col_b = st.columns(2)
    with col_a:
        st.write("**1. Dataset (10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)**")
        st.caption("‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Synthetic Netflix Thai Reviews ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 5,000 ‡πÅ‡∏ñ‡∏ß ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô 3 ‡∏Ñ‡∏•‡∏≤‡∏™ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå")
    with col_b:
        st.write("**2. Preprocessing (10 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô)**")
        st.caption("‡πÉ‡∏ä‡πâ PyThaiNLP 'newmm' ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Tokenization ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏î‡πâ‡∏ß‡∏¢ TF-IDF Vectorizer")

# --- 4. MAIN INTERFACE ---
if 'h' not in st.session_state: st.session_state.update({'h':'', 'b':'', 'l':'Positive'})

c1, c2 = st.columns([2, 1])

with c1:
    st.subheader("‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏µ‡∏ß‡∏¥‡∏ß")
    if st.button("‡∏™‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏µ‡∏ß‡∏¥‡∏ß (Random)"):
        if df is not None:
            s = df.sample(1).iloc[0]
            st.session_state.update({'h': f"ID-{s['review_id'][:5]}", 'b': s['text'], 'l': s['label']})
            st.rerun()
    
    headline = st.text_input("‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠/ID", value=st.session_state.h)
    body = st.text_area("‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏£‡∏µ‡∏ß‡∏¥‡∏ß", value=st.session_state.b, height=150)
    
    if st.button("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (Execute)", type="primary"):
        if body.strip():
            res1, res2 = st.columns(2)
            for m, col, name in [(model_v1, res1, "Model V1 (Baseline)"), (model_v2, res2, "Model V2 (Optimized)")]:
                with col:
                    if m:
                        pred = m.predict([f"{headline} {body}"])[0]
                        st.info(f"**{name}**")
                        st.markdown(f"### ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: `{pred}`")
                    else: st.error("‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•")

with c2:
    st.subheader("üí¨ ‡∏£‡∏∞‡∏ö‡∏ö‡∏ñ‡∏≤‡∏°-‡∏ï‡∏≠‡∏ö")
    if "messages" not in st.session_state: st.session_state.messages = []
    
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]): st.markdown(msg["content"])

    if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"): st.markdown(prompt)
        with st.chat_message("assistant"):
            st.markdown("‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö")
            st.session_state.messages.append({"role": "assistant", "content": "‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö"})

# --- 5. EVALUATION METRICS (15 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô) ---
st.divider()
st.subheader("üìà ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• (Evaluation)")
m1, m2, m3 = st.columns(3)
m1.metric("Dataset Size", "5,000 rows")
m2.metric("Preprocessing", "TF-IDF + newmm")
m3.metric("Overall Accuracy", "~99%")
